{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOA16Q3P/4BzQhiy56YQlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samson12193/Ingestion_and_pipeline/blob/main/testutility.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elqd-zmHqk_4"
      },
      "outputs": [],
      "source": [
        " # Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import modin.pandas as pdm\n",
        "import time\n",
        "import os\n",
        "import io\n",
        "from dask import dataframe as df1\n",
        "import dask.dataframe as dd\n",
        "import logging\n",
        "import subprocess\n",
        "import datetime\n",
        "import gc\n",
        "import re\n",
        "import yaml\n",
        "\n",
        "%%writefile testutility.py\n",
        "\n",
        "# Util.py file Reading function\n",
        "# This script contains utility functions for reading configuration files\n",
        "# and validating DataFrame columns.\n",
        "\n",
        "def read_config_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads a YAML configuration file and returns the contents.\n",
        "\n",
        "    Parameters:\n",
        "    filepath (str): Path to the YAML file.\n",
        "\n",
        "    Returns:\n",
        "    dict: Contents of the YAML file as a dictionary, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open and read the YAML file\n",
        "        with open(filepath, 'r') as stream:\n",
        "            return yaml.safe_load(stream)  # Parse and return the YAML file contents\n",
        "    except yaml.YAMLError as exc:\n",
        "        # Print an error message if a YAML parsing error occurs\n",
        "        print(exc)\n",
        "        return None\n",
        "    except FileNotFoundError:\n",
        "        # Print an error message if the file is not found\n",
        "        print(\"File not found:\", filepath)\n",
        "        return None\n",
        "\n",
        "# Using the read_config_file function\n",
        "# This reads the configuration data from the specified YAML file\n",
        "config_data = read_config_file(\"ingested_file_info.yaml\")\n",
        "\n",
        "def col_header_val(df, table_config):\n",
        "    \"\"\"\n",
        "    Validates and standardizes the column names of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame whose columns are to be validated.\n",
        "    table_config (dict): Configuration dictionary containing expected column names.\n",
        "\n",
        "    Returns:\n",
        "    int: 1 if validation passes, 0 if validation fails.\n",
        "    \"\"\"\n",
        "    # Convert all column names to lowercase\n",
        "    df.columns = df.columns.str.lower()\n",
        "\n",
        "    # Replace non-alphanumeric characters with underscores\n",
        "    df.columns = df.columns.str.replace('[^\\w]', '_', regex=True)\n",
        "\n",
        "    # Remove leading and trailing underscores from column names\n",
        "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
        "\n",
        "    # Get the list of expected column names from the configuration\n",
        "    expected_col = list(map(lambda x: x.lower(), table_config['columns']))\n",
        "    expected_col.sort()  # Sort the expected column names\n",
        "\n",
        "    # Sort DataFrame columns for comparison\n",
        "    df.columns = list(map(lambda x: x.lower(), list(df.columns)))\n",
        "    df = df.reindex(sorted(df.columns), axis=1)\n",
        "\n",
        "    # Check if the DataFrame columns match the expected columns\n",
        "    if len(df.columns) == len(expected_col) and list(expected_col) == list(df.columns):\n",
        "        print(\"column name and column length validation passed\")\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"column name and column length validation failed\")\n",
        "\n",
        "        # Identify and print mismatched columns\n",
        "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
        "        print(\"Following File columns are not in the YAML file:\", mismatched_columns_file)\n",
        "\n",
        "        # Identify and print missing columns\n",
        "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
        "        print(\"Following YAML columns are not in the file uploaded:\", missing_YAML_file)\n",
        "\n",
        "        # Log detailed information about the columns\n",
        "        logging.info(f'df columns: {df.columns}')\n",
        "        logging.info(f'expected columns: {expected_col}')\n",
        "\n",
        "        return 0"
      ]
    }
  ]
}